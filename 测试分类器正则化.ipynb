{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/nfs/xwx/model-doctor-xwx')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import models\n",
    "import loaders\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from configs import config\n",
    "from utils.lr_util import get_lr_scheduler\n",
    "from utils.time_util import print_time, get_current_time\n",
    "from sklearn.metrics import classification_report\n",
    "from loss.refl import reduce_equalized_focal_loss\n",
    "from loss.fl import focal_loss\n",
    "from loss.hcl import hc_loss\n",
    "from modify_kernel.util.draw_util import draw_lr, draw_fc_weight\n",
    "from modify_kernel.util.cfg_util import print_yml_cfg\n",
    "from functools import partial\n",
    "from utils.args_util import print_args\n",
    "from utils.general import init_seeds, get_head_and_kernel, get_head_ratio\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings # ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet32\"\n",
    "# model_path = \"/nfs/xwx/model-doctor-xwx/output/model/pretrained/resnet32/cifar-100-lt-ir100/lr0.1/custom_lr_scheduler/ce_loss/2022-07-01_04-04-08/best-model-acc0.3912.pth\"\n",
    "# model_path = \"/nfs/xwx/model-doctor-xwx/output/model/pretrained/resnet32/cifar-100-lt-ir10/lr0.1/custom_lr_scheduler/ce_loss/2022-07-06_21-16-55/best-model-acc0.5810.pth\"\n",
    "model_path = \"/nfs/xwx/model-doctor-xwx/output/model/pretrained/resnet32/cifar-10-lt-ir100/lr0.1/custom_lr_scheduler/bsl_loss/2022-07-14_21-13-14/best-model-acc0.8245.pth\"\n",
    "data_name  = \"cifar-10-lt-ir100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders, _ = loaders.load_data(data_name=data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = models.load_model(\n",
    "    model_name=model_name, \n",
    "    in_channels=3,\n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "base_model.load_state_dict(torch.load(model_path)[\"model\"])\n",
    "base_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred, _ = model(X)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    correct /= size\n",
    "        \n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(data_loaders[\"val\"], base_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model = copy.deepcopy(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fc_weight(fc_weight):\n",
    "    fc_weight = np.sum(fc_weight * fc_weight, axis=1)\n",
    "    fc_weight = fc_weight**0.5\n",
    "\n",
    "    plt.plot(range(len(fc_weight)), fc_weight, 'r', label='fc weight')\n",
    "    \n",
    "    plt.title(\"fc weight\")\n",
    "    plt.xlabel(\"class\")\n",
    "    plt.ylabel(\"l2 weight\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norms(W, y_range=None):\n",
    "    # per-class weight norms vs. class cardinality\n",
    "    tmp = torch.linalg.norm(W, ord=2, dim=1).detach().numpy()\n",
    "    \n",
    "    if y_range==None:\n",
    "        max_val, mid_val, min_val = tmp.max(), tmp.mean(), tmp.min()\n",
    "        c = min(1/mid_val, mid_val)\n",
    "        y_range = [min_val-c, max_val+c]\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(15,3), dpi=64, facecolor='w', edgecolor='k')\n",
    "    plt.xticks(list(range(100)), rotation=90, fontsize=8);  # Set text labels.\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    ax1.set_ylabel('norm', fontsize=16)\n",
    "    ax1.set_ylim(y_range)\n",
    "    \n",
    "    plt.plot(tmp, linewidth=2)\n",
    "    plt.title('norms of per-class weights from the learned classifier vs. class cardinality', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_weight = base_model.linear.weight.detach().cpu().numpy()\n",
    "    \n",
    "draw_fc_weight(fc_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_norms(base_model.linear.weight.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(): \n",
    "    def __init__(self, LpNorm=2, tau = 1):\n",
    "        self.LpNorm = LpNorm\n",
    "        self.tau = tau\n",
    "  \n",
    "    def apply_on(self, model): #this method applies tau-normalization on the classifier layer\n",
    "\n",
    "        for curLayer in [model.linear.weight]: #change to last layer: Done\n",
    "            curparam = curLayer.data\n",
    "\n",
    "            curparam_vec = curparam.reshape((curparam.shape[0], -1))\n",
    "            neuronNorm_curparam = (torch.linalg.norm(curparam_vec, ord=self.LpNorm, dim=1)**self.tau).detach().unsqueeze(-1)\n",
    "            scalingVect = torch.ones_like(curparam)    \n",
    "            \n",
    "            idx = neuronNorm_curparam == neuronNorm_curparam\n",
    "            idx = idx.squeeze()\n",
    "            tmp = 1 / (neuronNorm_curparam[idx].squeeze())\n",
    "            for _ in range(len(scalingVect.shape)-1):\n",
    "                tmp = tmp.unsqueeze(-1)\n",
    "\n",
    "            scalingVect[idx] = torch.mul(scalingVect[idx], tmp)\n",
    "            curparam[idx] = scalingVect[idx] * curparam[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "L2_norm_model = copy.deepcopy(base_model)\n",
    "L2_norm = Normalizer(tau=1)\n",
    "L2_norm.apply_on(L2_norm_model)\n",
    "\n",
    "test(data_loaders[\"val\"], L2_norm_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_norms(L2_norm_model.linear.weight.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = [0.1*i for i in range(1, 21, 1)]\n",
    "\n",
    "for tau in taus:\n",
    "    print(f\"tau: {tau:.1f}, \",end=\"\")\n",
    "    tau_norm_model = copy.deepcopy(base_model)\n",
    "    tau_norm = Normalizer(tau=tau)\n",
    "    tau_norm.apply_on(tau_norm_model)\n",
    "\n",
    "\n",
    "    test(data_loaders[\"val\"], tau_norm_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_norms(tau_norm_model.linear.weight.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_norm_model = copy.deepcopy(base_model)\n",
    "fc_weight = L2_norm_model.linear.weight\n",
    "L2_norm_model.linear.weight.data = fc_weight / torch.norm(fc_weight, 2, 1, keepdim=True)\n",
    "\n",
    "test(data_loaders[\"val\"], L2_norm_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    train_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            pred, _ = model(X)  # 网络前向计算\n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()  # 清除过往梯度\n",
    "            loss.backward()  # 得到模型中参数对当前输入的梯度\n",
    "            optimizer.step()  # 更新参数\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Train Error: Accuracy: {(100*correct):>0.2f}%, Avg loss: {train_loss:>8f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "# 随机化FC层参数\n",
    "# fc_weight = model.linear.weight\n",
    "# stdv = 1. / math.sqrt(fc_weight.size(1))\n",
    "# model.linear.weight.data.uniform_(-stdv, stdv)\n",
    "model = copy.deepcopy(base_model)\n",
    "\n",
    "# fc_weight = model.linear.weight\n",
    "# 随机化分类头的值重新训练\n",
    "# stdv = 1. / math.sqrt(fc_weight.size(1))\n",
    "# model.linear.weight.data.uniform_(-stdv, stdv)\n",
    "# model.linear.weight.data = fc_weight / torch.norm(fc_weight, 2, 1, keepdim=True)\n",
    "\n",
    "active_layers = [model.linear.weight, model.linear.bias]  # 只调整分类头\n",
    "# active_layers = [model.layer3[4].conv2.weight, model.linear.weight, model.linear.bias]\n",
    "\n",
    "for param in model.parameters(): #freez all model paramters except the classifier layer\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in active_layers:\n",
    "    param.requires_grad = True\n",
    "\n",
    "base_lr = 0.01\n",
    "total_epoch_num = 10\n",
    "weight_decay = 2e-3 #weight decay value\n",
    "\n",
    "optimizer = optim.SGD([{'params': active_layers, 'lr': base_lr}], lr=base_lr, momentum=0.9, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, total_epoch_num, eta_min=0.0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(total_epoch_num):\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    train(data_loaders[\"train\"], model, loss_fn, optimizer, device)\n",
    "    test(data_loaders[\"val\"], model, device)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4adb7e4a57e7e3d856012932a35d4e129d3fdf13aea9525c60137481fe376969"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('modelDoctor': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
