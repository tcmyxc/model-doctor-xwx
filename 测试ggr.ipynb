{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, qkv_bias=False, qk_scale=None, attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.scale = qk_scale or dim ** -0.5\n",
    "        # self.norm1q = nn.LayerNorm(dim)\n",
    "        # self.norm1k = nn.LayerNorm(dim)\n",
    "\n",
    "        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "    def forward(self, qx, kx):\n",
    "        qx = qx.unsqueeze(1)\n",
    "        kx = kx if len(kx.shape) == 3 else kx.unsqueeze(1)\n",
    "        # qx:[Bq, 1, C]\n",
    "        # kx:[Bk, Nk, C]\n",
    "        assert qx.shape[-1] == kx.shape[-1] and qx.shape[1] == 1\n",
    "\n",
    "        # q = self.wq(self.norm1q(qx))\n",
    "        # k = self.wk(self.norm1k(kx))\n",
    "        q = self.wq(qx)\n",
    "        k = self.wk(kx)\n",
    "        v = kx\n",
    "        attn = torch.einsum('qoc,knc->qkn', q, k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "        x = torch.einsum('knc,qkn->qkc', v, attn)\n",
    "\n",
    "        idx = cos(qx, x).argmax(-1)\n",
    "        return x[:, idx, :][0]\n",
    "\n",
    "\n",
    "class GGR(nn.Module):\n",
    "    def __init__(self, dim=64):\n",
    "        super().__init__()\n",
    "        self.mlp = Mlp(dim)\n",
    "        self.attn = Attention(dim)\n",
    "\n",
    "    def forward(self, x, kx):\n",
    "        out1 = self.mlp(x)\n",
    "        out2 = self.attn(x, kx)\n",
    "\n",
    "        return out1 + out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Properly implemented ResNet-s for CIFAR10 as described in paper [1].\n",
    "\n",
    "The implementation and structure of this file is hugely influenced by [2]\n",
    "which is implemented for ImageNet and doesn't have option A for identity.\n",
    "Moreover, most of the implementations on the web is copy-paste from\n",
    "torchvision's resnet and has wrong number of params.\n",
    "\n",
    "Proper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\n",
    "number of layers and parameters:\n",
    "\n",
    "name      | layers | params\n",
    "ResNet20  |    20  | 0.27M\n",
    "ResNet32  |    32  | 0.46M\n",
    "ResNet44  |    44  | 0.66M\n",
    "ResNet56  |    56  | 0.85M\n",
    "ResNet110 |   110  |  1.7M\n",
    "ResNet1202|  1202  | 19.4m\n",
    "\n",
    "which this implementation indeed has.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "If you use this implementation in you work, please don't forget to mention the\n",
    "author, Yerlan Idelbayev.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channels=3, num_classes=10, use_ggr=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        self.ggr = GGR(64)\n",
    "        self.use_ggr = use_ggr\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, kx):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)  # b x 64\n",
    "        \n",
    "        if self.use_ggr:\n",
    "            out = self.ggr(out, kx)\n",
    "            \n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32(in_channels=3, num_classes=10, use_ggr=False):\n",
    "    return ResNet(BasicBlock, [5, 5, 5], in_channels, num_classes, use_ggr)\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/nfs/xwx/model-doctor-xwx')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import models\n",
    "import loaders\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from configs import config\n",
    "from utils.lr_util import get_lr_scheduler\n",
    "from utils.time_util import print_time, get_current_time\n",
    "from sklearn.metrics import classification_report\n",
    "from loss.refl import reduce_equalized_focal_loss\n",
    "from loss.fl import focal_loss\n",
    "from loss.hcl import hc_loss\n",
    "from modify_kernel.util.draw_util import draw_lr, draw_fc_weight\n",
    "from modify_kernel.util.cfg_util import print_yml_cfg\n",
    "from functools import partial\n",
    "from utils.args_util import print_args\n",
    "from utils.general import init_seeds, get_head_and_kernel, get_head_ratio\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings # ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/nfs/xwx/model-doctor-xwx/output/model/pretrained/resnet32/cifar-10-lt-ir100/lr0.01/cosine_lr_scheduler/ce_loss/2022-07-15_17-27-58/best-model-acc0.7144.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet32\"\n",
    "data_name  = \"cifar-10-lt-ir100\"\n",
    "\n",
    "grad_path = \"/nfs/xwx/model-doctor-xwx/output/result/channels/resnet32-cifar-10-lt-ir100/channel_grads_-1.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kx = torch.from_numpy(np.asarray(np.load(grad_path)))\n",
    "kx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "LOAD DATA: cifar-10-lt-ir100\n",
      "----------------------------------------\n",
      "load cifar dataset from image dir\n",
      "\n",
      "load cifar dataset from image dir\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_loaders, _ = loaders.load_data(data_name=data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = resnet32()\n",
    "\n",
    "base_model.load_state_dict(torch.load(model_path)[\"model\"], strict=False)\n",
    "base_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, kx, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        kx = kx.to(device)\n",
    "        pred = model(X, kx)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    correct /= size\n",
    "        \n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: Accuracy: 71.44%\n"
     ]
    }
   ],
   "source": [
    "test(data_loaders[\"val\"], base_model, kx, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, kx, device):\n",
    "    train_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        kx = kx.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            pred= model(X, kx)  # 网络前向计算\n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()  # 清除过往梯度\n",
    "            loss.backward()  # 得到模型中参数对当前输入的梯度\n",
    "            optimizer.step()  # 更新参数\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Train Error: Accuracy: {(100*correct):>0.2f}%, Avg loss: {train_loss:>8f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: Accuracy: 5.23%\n"
     ]
    }
   ],
   "source": [
    "base_model = resnet32(use_ggr=True)\n",
    "\n",
    "base_model.load_state_dict(torch.load(model_path)[\"model\"], strict=False)\n",
    "base_model.to(device)\n",
    "\n",
    "test(data_loaders[\"val\"], base_model, kx, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "model = copy.deepcopy(base_model)\n",
    "\n",
    "# active_layers = [model.linear.weight, model.linear.bias]  \n",
    "\n",
    "# for param in model.parameters(): #freez all model paramters except the classifier layer\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# for param in active_layers:\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for param in model.ggr.parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "# for param in model.layer3[4].conv2.parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad == True:\n",
    "#         print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Train Error: Accuracy: 88.49%, Avg loss: 0.354456\n",
      "Test Error: Accuracy: 52.81%\n",
      "\n",
      "Epoch 2\n",
      "Train Error: Accuracy: 93.75%, Avg loss: 0.180768\n",
      "Test Error: Accuracy: 56.46%\n",
      "\n",
      "Epoch 3\n",
      "Train Error: Accuracy: 94.51%, Avg loss: 0.157835\n",
      "Test Error: Accuracy: 55.86%\n",
      "\n",
      "Epoch 4\n",
      "Train Error: Accuracy: 94.57%, Avg loss: 0.162109\n",
      "Test Error: Accuracy: 58.70%\n",
      "\n",
      "Epoch 5\n",
      "Train Error: Accuracy: 95.20%, Avg loss: 0.136385\n",
      "Test Error: Accuracy: 57.99%\n",
      "\n",
      "Epoch 6\n",
      "Train Error: Accuracy: 94.79%, Avg loss: 0.142090\n",
      "Test Error: Accuracy: 59.74%\n",
      "\n",
      "Epoch 7\n",
      "Train Error: Accuracy: 95.31%, Avg loss: 0.138194\n",
      "Test Error: Accuracy: 62.47%\n",
      "\n",
      "Epoch 8\n",
      "Train Error: Accuracy: 94.88%, Avg loss: 0.144692\n",
      "Test Error: Accuracy: 66.53%\n",
      "\n",
      "Epoch 9\n",
      "Train Error: Accuracy: 94.82%, Avg loss: 0.142463\n",
      "Test Error: Accuracy: 65.39%\n",
      "\n",
      "Epoch 10\n",
      "Train Error: Accuracy: 95.43%, Avg loss: 0.128805\n",
      "Test Error: Accuracy: 64.62%\n",
      "\n",
      "Epoch 11\n",
      "Train Error: Accuracy: 95.70%, Avg loss: 0.122724\n",
      "Test Error: Accuracy: 61.45%\n",
      "\n",
      "Epoch 12\n",
      "Train Error: Accuracy: 95.09%, Avg loss: 0.144320\n",
      "Test Error: Accuracy: 63.62%\n",
      "\n",
      "Epoch 13\n",
      "Train Error: Accuracy: 95.63%, Avg loss: 0.122627\n",
      "Test Error: Accuracy: 59.18%\n",
      "\n",
      "Epoch 14\n",
      "Train Error: Accuracy: 95.90%, Avg loss: 0.117149\n",
      "Test Error: Accuracy: 61.82%\n",
      "\n",
      "Epoch 15\n",
      "Train Error: Accuracy: 96.09%, Avg loss: 0.112374\n",
      "Test Error: Accuracy: 69.00%\n",
      "\n",
      "Epoch 16\n",
      "Train Error: Accuracy: 96.23%, Avg loss: 0.109988\n",
      "Test Error: Accuracy: 61.47%\n",
      "\n",
      "Epoch 17\n",
      "Train Error: Accuracy: 96.05%, Avg loss: 0.113505\n",
      "Test Error: Accuracy: 61.99%\n",
      "\n",
      "Epoch 18\n",
      "Train Error: Accuracy: 96.30%, Avg loss: 0.108968\n",
      "Test Error: Accuracy: 61.41%\n",
      "\n",
      "Epoch 19\n",
      "Train Error: Accuracy: 96.21%, Avg loss: 0.106130\n",
      "Test Error: Accuracy: 67.57%\n",
      "\n",
      "Epoch 20\n",
      "Train Error: Accuracy: 96.61%, Avg loss: 0.097701\n",
      "Test Error: Accuracy: 65.24%\n",
      "\n",
      "Epoch 21\n",
      "Train Error: Accuracy: 96.74%, Avg loss: 0.097688\n",
      "Test Error: Accuracy: 62.28%\n",
      "\n",
      "Epoch 22\n",
      "Train Error: Accuracy: 96.51%, Avg loss: 0.102175\n",
      "Test Error: Accuracy: 64.04%\n",
      "\n",
      "Epoch 23\n",
      "Train Error: Accuracy: 96.61%, Avg loss: 0.097398\n",
      "Test Error: Accuracy: 58.83%\n",
      "\n",
      "Epoch 24\n",
      "Train Error: Accuracy: 96.36%, Avg loss: 0.103184\n",
      "Test Error: Accuracy: 61.67%\n",
      "\n",
      "Epoch 25\n",
      "Train Error: Accuracy: 97.07%, Avg loss: 0.086627\n",
      "Test Error: Accuracy: 59.83%\n",
      "\n",
      "Epoch 26\n",
      "Train Error: Accuracy: 96.96%, Avg loss: 0.085616\n",
      "Test Error: Accuracy: 57.95%\n",
      "\n",
      "Epoch 27\n",
      "Train Error: Accuracy: 96.83%, Avg loss: 0.086792\n",
      "Test Error: Accuracy: 66.09%\n",
      "\n",
      "Epoch 28\n",
      "Train Error: Accuracy: 96.78%, Avg loss: 0.090566\n",
      "Test Error: Accuracy: 63.72%\n",
      "\n",
      "Epoch 29\n",
      "Train Error: Accuracy: 96.81%, Avg loss: 0.094890\n",
      "Test Error: Accuracy: 59.40%\n",
      "\n",
      "Epoch 30\n",
      "Train Error: Accuracy: 97.42%, Avg loss: 0.076394\n",
      "Test Error: Accuracy: 63.64%\n",
      "\n",
      "Epoch 31\n",
      "Train Error: Accuracy: 97.65%, Avg loss: 0.069222\n",
      "Test Error: Accuracy: 62.26%\n",
      "\n",
      "Epoch 32\n",
      "Train Error: Accuracy: 97.24%, Avg loss: 0.078575\n",
      "Test Error: Accuracy: 57.48%\n",
      "\n",
      "Epoch 33\n",
      "Train Error: Accuracy: 97.17%, Avg loss: 0.081770\n",
      "Test Error: Accuracy: 63.17%\n",
      "\n",
      "Epoch 34\n",
      "Train Error: Accuracy: 97.13%, Avg loss: 0.081653\n",
      "Test Error: Accuracy: 66.27%\n",
      "\n",
      "Epoch 35\n",
      "Train Error: Accuracy: 97.28%, Avg loss: 0.076553\n",
      "Test Error: Accuracy: 68.06%\n",
      "\n",
      "Epoch 36\n",
      "Train Error: Accuracy: 97.57%, Avg loss: 0.067672\n",
      "Test Error: Accuracy: 61.44%\n",
      "\n",
      "Epoch 37\n",
      "Train Error: Accuracy: 97.67%, Avg loss: 0.066652\n",
      "Test Error: Accuracy: 69.66%\n",
      "\n",
      "Epoch 38\n",
      "Train Error: Accuracy: 97.77%, Avg loss: 0.064784\n",
      "Test Error: Accuracy: 62.54%\n",
      "\n",
      "Epoch 39\n",
      "Train Error: Accuracy: 97.49%, Avg loss: 0.067782\n",
      "Test Error: Accuracy: 66.68%\n",
      "\n",
      "Epoch 40\n",
      "Train Error: Accuracy: 97.77%, Avg loss: 0.062750\n",
      "Test Error: Accuracy: 65.76%\n",
      "\n",
      "Epoch 41\n",
      "Train Error: Accuracy: 97.94%, Avg loss: 0.059479\n",
      "Test Error: Accuracy: 65.25%\n",
      "\n",
      "Epoch 42\n",
      "Train Error: Accuracy: 97.53%, Avg loss: 0.069923\n",
      "Test Error: Accuracy: 62.69%\n",
      "\n",
      "Epoch 43\n",
      "Train Error: Accuracy: 97.94%, Avg loss: 0.060381\n",
      "Test Error: Accuracy: 60.92%\n",
      "\n",
      "Epoch 44\n",
      "Train Error: Accuracy: 97.42%, Avg loss: 0.069976\n",
      "Test Error: Accuracy: 68.20%\n",
      "\n",
      "Epoch 45\n",
      "Train Error: Accuracy: 98.26%, Avg loss: 0.050313\n",
      "Test Error: Accuracy: 67.65%\n",
      "\n",
      "Epoch 46\n",
      "Train Error: Accuracy: 98.19%, Avg loss: 0.052071\n",
      "Test Error: Accuracy: 68.15%\n",
      "\n",
      "Epoch 47\n",
      "Train Error: Accuracy: 98.19%, Avg loss: 0.051875\n",
      "Test Error: Accuracy: 67.61%\n",
      "\n",
      "Epoch 48\n",
      "Train Error: Accuracy: 98.04%, Avg loss: 0.056803\n",
      "Test Error: Accuracy: 65.58%\n",
      "\n",
      "Epoch 49\n",
      "Train Error: Accuracy: 98.29%, Avg loss: 0.048970\n",
      "Test Error: Accuracy: 67.66%\n",
      "\n",
      "Epoch 50\n",
      "Train Error: Accuracy: 98.34%, Avg loss: 0.048194\n",
      "Test Error: Accuracy: 67.18%\n",
      "\n",
      "Epoch 51\n",
      "Train Error: Accuracy: 97.98%, Avg loss: 0.055081\n",
      "Test Error: Accuracy: 69.82%\n",
      "\n",
      "Epoch 52\n",
      "Train Error: Accuracy: 97.78%, Avg loss: 0.060612\n",
      "Test Error: Accuracy: 65.85%\n",
      "\n",
      "Epoch 53\n",
      "Train Error: Accuracy: 98.33%, Avg loss: 0.048379\n",
      "Test Error: Accuracy: 67.89%\n",
      "\n",
      "Epoch 54\n",
      "Train Error: Accuracy: 98.31%, Avg loss: 0.048293\n",
      "Test Error: Accuracy: 62.49%\n",
      "\n",
      "Epoch 55\n",
      "Train Error: Accuracy: 98.56%, Avg loss: 0.043060\n",
      "Test Error: Accuracy: 66.96%\n",
      "\n",
      "Epoch 56\n",
      "Train Error: Accuracy: 98.51%, Avg loss: 0.045484\n",
      "Test Error: Accuracy: 65.39%\n",
      "\n",
      "Epoch 57\n",
      "Train Error: Accuracy: 98.53%, Avg loss: 0.043101\n",
      "Test Error: Accuracy: 67.41%\n",
      "\n",
      "Epoch 58\n",
      "Train Error: Accuracy: 98.85%, Avg loss: 0.034024\n",
      "Test Error: Accuracy: 64.31%\n",
      "\n",
      "Epoch 59\n",
      "Train Error: Accuracy: 98.93%, Avg loss: 0.033293\n",
      "Test Error: Accuracy: 66.66%\n",
      "\n",
      "Epoch 60\n",
      "Train Error: Accuracy: 98.61%, Avg loss: 0.038856\n",
      "Test Error: Accuracy: 68.53%\n",
      "\n",
      "Epoch 61\n",
      "Train Error: Accuracy: 98.95%, Avg loss: 0.032575\n",
      "Test Error: Accuracy: 69.54%\n",
      "\n",
      "Epoch 62\n",
      "Train Error: Accuracy: 98.55%, Avg loss: 0.038972\n",
      "Test Error: Accuracy: 61.09%\n",
      "\n",
      "Epoch 63\n",
      "Train Error: Accuracy: 98.06%, Avg loss: 0.053443\n",
      "Test Error: Accuracy: 57.65%\n",
      "\n",
      "Epoch 64\n",
      "Train Error: Accuracy: 98.60%, Avg loss: 0.039731\n",
      "Test Error: Accuracy: 69.55%\n",
      "\n",
      "Epoch 65\n",
      "Train Error: Accuracy: 98.69%, Avg loss: 0.039389\n",
      "Test Error: Accuracy: 68.61%\n",
      "\n",
      "Epoch 66\n",
      "Train Error: Accuracy: 98.78%, Avg loss: 0.033909\n",
      "Test Error: Accuracy: 63.53%\n",
      "\n",
      "Epoch 67\n",
      "Train Error: Accuracy: 98.69%, Avg loss: 0.037630\n",
      "Test Error: Accuracy: 67.77%\n",
      "\n",
      "Epoch 68\n",
      "Train Error: Accuracy: 98.76%, Avg loss: 0.034309\n",
      "Test Error: Accuracy: 66.67%\n",
      "\n",
      "Epoch 69\n",
      "Train Error: Accuracy: 99.02%, Avg loss: 0.029411\n",
      "Test Error: Accuracy: 68.12%\n",
      "\n",
      "Epoch 70\n",
      "Train Error: Accuracy: 98.81%, Avg loss: 0.034256\n",
      "Test Error: Accuracy: 59.78%\n",
      "\n",
      "Epoch 71\n",
      "Train Error: Accuracy: 98.37%, Avg loss: 0.047245\n",
      "Test Error: Accuracy: 67.57%\n",
      "\n",
      "Epoch 72\n",
      "Train Error: Accuracy: 98.90%, Avg loss: 0.029781\n",
      "Test Error: Accuracy: 63.61%\n",
      "\n",
      "Epoch 73\n",
      "Train Error: Accuracy: 99.11%, Avg loss: 0.029868\n",
      "Test Error: Accuracy: 65.99%\n",
      "\n",
      "Epoch 74\n",
      "Train Error: Accuracy: 99.08%, Avg loss: 0.026768\n",
      "Test Error: Accuracy: 64.56%\n",
      "\n",
      "Epoch 75\n",
      "Train Error: Accuracy: 99.24%, Avg loss: 0.022328\n",
      "Test Error: Accuracy: 66.59%\n",
      "\n",
      "Epoch 76\n",
      "Train Error: Accuracy: 99.40%, Avg loss: 0.018218\n",
      "Test Error: Accuracy: 71.30%\n",
      "\n",
      "Epoch 77\n",
      "Train Error: Accuracy: 99.52%, Avg loss: 0.016691\n",
      "Test Error: Accuracy: 67.89%\n",
      "\n",
      "Epoch 78\n",
      "Train Error: Accuracy: 99.42%, Avg loss: 0.018858\n",
      "Test Error: Accuracy: 65.51%\n",
      "\n",
      "Epoch 79\n",
      "Train Error: Accuracy: 99.14%, Avg loss: 0.024903\n",
      "Test Error: Accuracy: 64.38%\n",
      "\n",
      "Epoch 80\n",
      "Train Error: Accuracy: 99.40%, Avg loss: 0.019345\n",
      "Test Error: Accuracy: 65.97%\n",
      "\n",
      "Epoch 81\n",
      "Train Error: Accuracy: 99.22%, Avg loss: 0.022380\n",
      "Test Error: Accuracy: 66.02%\n",
      "\n",
      "Epoch 82\n",
      "Train Error: Accuracy: 99.23%, Avg loss: 0.021617\n",
      "Test Error: Accuracy: 67.74%\n",
      "\n",
      "Epoch 83\n",
      "Train Error: Accuracy: 99.36%, Avg loss: 0.020322\n",
      "Test Error: Accuracy: 68.57%\n",
      "\n",
      "Epoch 84\n",
      "Train Error: Accuracy: 99.38%, Avg loss: 0.019850\n",
      "Test Error: Accuracy: 65.57%\n",
      "\n",
      "Epoch 85\n",
      "Train Error: Accuracy: 99.52%, Avg loss: 0.014749\n",
      "Test Error: Accuracy: 67.96%\n",
      "\n",
      "Epoch 86\n",
      "Train Error: Accuracy: 99.44%, Avg loss: 0.017621\n",
      "Test Error: Accuracy: 67.27%\n",
      "\n",
      "Epoch 87\n",
      "Train Error: Accuracy: 99.51%, Avg loss: 0.017078\n",
      "Test Error: Accuracy: 66.64%\n",
      "\n",
      "Epoch 88\n",
      "Train Error: Accuracy: 99.48%, Avg loss: 0.015357\n",
      "Test Error: Accuracy: 66.15%\n",
      "\n",
      "Epoch 89\n",
      "Train Error: Accuracy: 99.49%, Avg loss: 0.015342\n",
      "Test Error: Accuracy: 67.23%\n",
      "\n",
      "Epoch 90\n",
      "Train Error: Accuracy: 99.44%, Avg loss: 0.015366\n",
      "Test Error: Accuracy: 67.79%\n",
      "\n",
      "Epoch 91\n",
      "Train Error: Accuracy: 99.56%, Avg loss: 0.012647\n",
      "Test Error: Accuracy: 65.28%\n",
      "\n",
      "Epoch 92\n",
      "Train Error: Accuracy: 99.61%, Avg loss: 0.012035\n",
      "Test Error: Accuracy: 65.87%\n",
      "\n",
      "Epoch 93\n",
      "Train Error: Accuracy: 99.77%, Avg loss: 0.008513\n",
      "Test Error: Accuracy: 65.67%\n",
      "\n",
      "Epoch 94\n",
      "Train Error: Accuracy: 99.75%, Avg loss: 0.008462\n",
      "Test Error: Accuracy: 66.83%\n",
      "\n",
      "Epoch 95\n",
      "Train Error: Accuracy: 99.81%, Avg loss: 0.006897\n",
      "Test Error: Accuracy: 67.45%\n",
      "\n",
      "Epoch 96\n",
      "Train Error: Accuracy: 99.75%, Avg loss: 0.008516\n",
      "Test Error: Accuracy: 68.68%\n",
      "\n",
      "Epoch 97\n",
      "Train Error: Accuracy: 99.75%, Avg loss: 0.008578\n",
      "Test Error: Accuracy: 65.59%\n",
      "\n",
      "Epoch 98\n",
      "Train Error: Accuracy: 99.76%, Avg loss: 0.007566\n",
      "Test Error: Accuracy: 67.59%\n",
      "\n",
      "Epoch 99\n",
      "Train Error: Accuracy: 99.72%, Avg loss: 0.009871\n",
      "Test Error: Accuracy: 66.53%\n",
      "\n",
      "Epoch 100\n",
      "Train Error: Accuracy: 99.69%, Avg loss: 0.010014\n",
      "Test Error: Accuracy: 65.00%\n",
      "\n",
      "Epoch 101\n",
      "Train Error: Accuracy: 99.77%, Avg loss: 0.007516\n",
      "Test Error: Accuracy: 69.46%\n",
      "\n",
      "Epoch 102\n",
      "Train Error: Accuracy: 99.91%, Avg loss: 0.004622\n",
      "Test Error: Accuracy: 67.61%\n",
      "\n",
      "Epoch 103\n",
      "Train Error: Accuracy: 99.87%, Avg loss: 0.005659\n",
      "Test Error: Accuracy: 66.05%\n",
      "\n",
      "Epoch 104\n",
      "Train Error: Accuracy: 99.88%, Avg loss: 0.004591\n",
      "Test Error: Accuracy: 66.41%\n",
      "\n",
      "Epoch 105\n",
      "Train Error: Accuracy: 99.85%, Avg loss: 0.006870\n",
      "Test Error: Accuracy: 67.44%\n",
      "\n",
      "Epoch 106\n",
      "Train Error: Accuracy: 99.75%, Avg loss: 0.006891\n",
      "Test Error: Accuracy: 68.19%\n",
      "\n",
      "Epoch 107\n",
      "Train Error: Accuracy: 99.85%, Avg loss: 0.005146\n",
      "Test Error: Accuracy: 67.96%\n",
      "\n",
      "Epoch 108\n",
      "Train Error: Accuracy: 99.85%, Avg loss: 0.006348\n",
      "Test Error: Accuracy: 66.44%\n",
      "\n",
      "Epoch 109\n",
      "Train Error: Accuracy: 99.91%, Avg loss: 0.003758\n",
      "Test Error: Accuracy: 67.09%\n",
      "\n",
      "Epoch 110\n",
      "Train Error: Accuracy: 99.91%, Avg loss: 0.004508\n",
      "Test Error: Accuracy: 66.68%\n",
      "\n",
      "Epoch 111\n",
      "Train Error: Accuracy: 99.93%, Avg loss: 0.004168\n",
      "Test Error: Accuracy: 67.78%\n",
      "\n",
      "Epoch 112\n",
      "Train Error: Accuracy: 99.85%, Avg loss: 0.005097\n",
      "Test Error: Accuracy: 66.65%\n",
      "\n",
      "Epoch 113\n",
      "Train Error: Accuracy: 99.93%, Avg loss: 0.003246\n",
      "Test Error: Accuracy: 66.71%\n",
      "\n",
      "Epoch 114\n",
      "Train Error: Accuracy: 99.92%, Avg loss: 0.003258\n",
      "Test Error: Accuracy: 69.11%\n",
      "\n",
      "Epoch 115\n",
      "Train Error: Accuracy: 99.94%, Avg loss: 0.002492\n",
      "Test Error: Accuracy: 66.51%\n",
      "\n",
      "Epoch 116\n",
      "Train Error: Accuracy: 99.94%, Avg loss: 0.003417\n",
      "Test Error: Accuracy: 66.51%\n",
      "\n",
      "Epoch 117\n",
      "Train Error: Accuracy: 99.92%, Avg loss: 0.003951\n",
      "Test Error: Accuracy: 67.77%\n",
      "\n",
      "Epoch 118\n",
      "Train Error: Accuracy: 99.95%, Avg loss: 0.002805\n",
      "Test Error: Accuracy: 67.96%\n",
      "\n",
      "Epoch 119\n",
      "Train Error: Accuracy: 99.96%, Avg loss: 0.002872\n",
      "Test Error: Accuracy: 68.67%\n",
      "\n",
      "Epoch 120\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.002308\n",
      "Test Error: Accuracy: 67.35%\n",
      "\n",
      "Epoch 121\n",
      "Train Error: Accuracy: 99.92%, Avg loss: 0.003252\n",
      "Test Error: Accuracy: 67.67%\n",
      "\n",
      "Epoch 122\n",
      "Train Error: Accuracy: 99.94%, Avg loss: 0.002840\n",
      "Test Error: Accuracy: 67.70%\n",
      "\n",
      "Epoch 123\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.002160\n",
      "Test Error: Accuracy: 67.40%\n",
      "\n",
      "Epoch 124\n",
      "Train Error: Accuracy: 99.94%, Avg loss: 0.002377\n",
      "Test Error: Accuracy: 68.94%\n",
      "\n",
      "Epoch 125\n",
      "Train Error: Accuracy: 99.94%, Avg loss: 0.002501\n",
      "Test Error: Accuracy: 68.11%\n",
      "\n",
      "Epoch 126\n",
      "Train Error: Accuracy: 99.95%, Avg loss: 0.002339\n",
      "Test Error: Accuracy: 67.10%\n",
      "\n",
      "Epoch 127\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.002046\n",
      "Test Error: Accuracy: 67.41%\n",
      "\n",
      "Epoch 128\n",
      "Train Error: Accuracy: 99.97%, Avg loss: 0.001963\n",
      "Test Error: Accuracy: 67.80%\n",
      "\n",
      "Epoch 129\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001615\n",
      "Test Error: Accuracy: 67.33%\n",
      "\n",
      "Epoch 130\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001448\n",
      "Test Error: Accuracy: 66.63%\n",
      "\n",
      "Epoch 131\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001508\n",
      "Test Error: Accuracy: 68.23%\n",
      "\n",
      "Epoch 132\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001562\n",
      "Test Error: Accuracy: 67.24%\n",
      "\n",
      "Epoch 133\n",
      "Train Error: Accuracy: 99.96%, Avg loss: 0.002186\n",
      "Test Error: Accuracy: 67.44%\n",
      "\n",
      "Epoch 134\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001736\n",
      "Test Error: Accuracy: 67.41%\n",
      "\n",
      "Epoch 135\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001643\n",
      "Test Error: Accuracy: 68.07%\n",
      "\n",
      "Epoch 136\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001808\n",
      "Test Error: Accuracy: 67.83%\n",
      "\n",
      "Epoch 137\n",
      "Train Error: Accuracy: 99.95%, Avg loss: 0.001764\n",
      "Test Error: Accuracy: 68.19%\n",
      "\n",
      "Epoch 138\n",
      "Train Error: Accuracy: 99.95%, Avg loss: 0.001766\n",
      "Test Error: Accuracy: 67.60%\n",
      "\n",
      "Epoch 139\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001476\n",
      "Test Error: Accuracy: 68.01%\n",
      "\n",
      "Epoch 140\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001636\n",
      "Test Error: Accuracy: 67.87%\n",
      "\n",
      "Epoch 141\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001085\n",
      "Test Error: Accuracy: 68.04%\n",
      "\n",
      "Epoch 142\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001215\n",
      "Test Error: Accuracy: 67.97%\n",
      "\n",
      "Epoch 143\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001165\n",
      "Test Error: Accuracy: 67.85%\n",
      "\n",
      "Epoch 144\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001477\n",
      "Test Error: Accuracy: 68.22%\n",
      "\n",
      "Epoch 145\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001241\n",
      "Test Error: Accuracy: 67.34%\n",
      "\n",
      "Epoch 146\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001432\n",
      "Test Error: Accuracy: 67.89%\n",
      "\n",
      "Epoch 147\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001153\n",
      "Test Error: Accuracy: 68.22%\n",
      "\n",
      "Epoch 148\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001339\n",
      "Test Error: Accuracy: 68.13%\n",
      "\n",
      "Epoch 149\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001386\n",
      "Test Error: Accuracy: 67.57%\n",
      "\n",
      "Epoch 150\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001304\n",
      "Test Error: Accuracy: 68.41%\n",
      "\n",
      "Epoch 151\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001157\n",
      "Test Error: Accuracy: 67.11%\n",
      "\n",
      "Epoch 152\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001087\n",
      "Test Error: Accuracy: 67.53%\n",
      "\n",
      "Epoch 153\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001060\n",
      "Test Error: Accuracy: 67.82%\n",
      "\n",
      "Epoch 154\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001341\n",
      "Test Error: Accuracy: 68.10%\n",
      "\n",
      "Epoch 155\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001182\n",
      "Test Error: Accuracy: 67.49%\n",
      "\n",
      "Epoch 156\n",
      "Train Error: Accuracy: 99.97%, Avg loss: 0.001434\n",
      "Test Error: Accuracy: 67.66%\n",
      "\n",
      "Epoch 157\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001063\n",
      "Test Error: Accuracy: 68.07%\n",
      "\n",
      "Epoch 158\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001481\n",
      "Test Error: Accuracy: 68.01%\n",
      "\n",
      "Epoch 159\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001182\n",
      "Test Error: Accuracy: 67.70%\n",
      "\n",
      "Epoch 160\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001256\n",
      "Test Error: Accuracy: 68.27%\n",
      "\n",
      "Epoch 161\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001180\n",
      "Test Error: Accuracy: 67.23%\n",
      "\n",
      "Epoch 162\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001142\n",
      "Test Error: Accuracy: 67.96%\n",
      "\n",
      "Epoch 163\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001130\n",
      "Test Error: Accuracy: 68.08%\n",
      "\n",
      "Epoch 164\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001322\n",
      "Test Error: Accuracy: 67.79%\n",
      "\n",
      "Epoch 165\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000890\n",
      "Test Error: Accuracy: 68.18%\n",
      "\n",
      "Epoch 166\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001352\n",
      "Test Error: Accuracy: 68.08%\n",
      "\n",
      "Epoch 167\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001123\n",
      "Test Error: Accuracy: 67.94%\n",
      "\n",
      "Epoch 168\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001153\n",
      "Test Error: Accuracy: 68.20%\n",
      "\n",
      "Epoch 169\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000940\n",
      "Test Error: Accuracy: 67.73%\n",
      "\n",
      "Epoch 170\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.000994\n",
      "Test Error: Accuracy: 67.88%\n",
      "\n",
      "Epoch 171\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001080\n",
      "Test Error: Accuracy: 68.02%\n",
      "\n",
      "Epoch 172\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001014\n",
      "Test Error: Accuracy: 68.11%\n",
      "\n",
      "Epoch 173\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001000\n",
      "Test Error: Accuracy: 67.70%\n",
      "\n",
      "Epoch 174\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001164\n",
      "Test Error: Accuracy: 67.82%\n",
      "\n",
      "Epoch 175\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001018\n",
      "Test Error: Accuracy: 67.46%\n",
      "\n",
      "Epoch 176\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001109\n",
      "Test Error: Accuracy: 67.96%\n",
      "\n",
      "Epoch 177\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001112\n",
      "Test Error: Accuracy: 67.73%\n",
      "\n",
      "Epoch 178\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.000995\n",
      "Test Error: Accuracy: 67.95%\n",
      "\n",
      "Epoch 179\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000962\n",
      "Test Error: Accuracy: 67.71%\n",
      "\n",
      "Epoch 180\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001127\n",
      "Test Error: Accuracy: 68.04%\n",
      "\n",
      "Epoch 181\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.000963\n",
      "Test Error: Accuracy: 68.33%\n",
      "\n",
      "Epoch 182\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001262\n",
      "Test Error: Accuracy: 68.14%\n",
      "\n",
      "Epoch 183\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000955\n",
      "Test Error: Accuracy: 67.74%\n",
      "\n",
      "Epoch 184\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000879\n",
      "Test Error: Accuracy: 67.97%\n",
      "\n",
      "Epoch 185\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000877\n",
      "Test Error: Accuracy: 67.76%\n",
      "\n",
      "Epoch 186\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001138\n",
      "Test Error: Accuracy: 67.18%\n",
      "\n",
      "Epoch 187\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001052\n",
      "Test Error: Accuracy: 67.75%\n",
      "\n",
      "Epoch 188\n",
      "Train Error: Accuracy: 99.98%, Avg loss: 0.001028\n",
      "Test Error: Accuracy: 68.13%\n",
      "\n",
      "Epoch 189\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000998\n",
      "Test Error: Accuracy: 67.86%\n",
      "\n",
      "Epoch 190\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.000968\n",
      "Test Error: Accuracy: 67.83%\n",
      "\n",
      "Epoch 191\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.000880\n",
      "Test Error: Accuracy: 67.61%\n",
      "\n",
      "Epoch 192\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000885\n",
      "Test Error: Accuracy: 68.00%\n",
      "\n",
      "Epoch 193\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000979\n",
      "Test Error: Accuracy: 68.06%\n",
      "\n",
      "Epoch 194\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001030\n",
      "Test Error: Accuracy: 67.60%\n",
      "\n",
      "Epoch 195\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000855\n",
      "Test Error: Accuracy: 67.91%\n",
      "\n",
      "Epoch 196\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000910\n",
      "Test Error: Accuracy: 68.23%\n",
      "\n",
      "Epoch 197\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.001328\n",
      "Test Error: Accuracy: 67.63%\n",
      "\n",
      "Epoch 198\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.000908\n",
      "Test Error: Accuracy: 67.64%\n",
      "\n",
      "Epoch 199\n",
      "Train Error: Accuracy: 100.00%, Avg loss: 0.001027\n",
      "Test Error: Accuracy: 67.62%\n",
      "\n",
      "Epoch 200\n",
      "Train Error: Accuracy: 99.99%, Avg loss: 0.000960\n",
      "Test Error: Accuracy: 67.41%\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "total_epoch_num = 200\n",
    "weight_decay = 5e-4 #weight decay value\n",
    "\n",
    "optimizer = optim.SGD(parameters, lr=base_lr, momentum=0.9, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, total_epoch_num, eta_min=0.0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(total_epoch_num):\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    train(data_loaders[\"train\"], model, loss_fn, optimizer, kx, device)\n",
    "    test(data_loaders[\"val\"], model, kx, device)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4adb7e4a57e7e3d856012932a35d4e129d3fdf13aea9525c60137481fe376969"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('modelDoctor': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
